# Projet OpenAIP - Pipeline de Donn√©es A√©ronautiques

## üìã Description

Pipeline Big Data pour le traitement et la visualisation de donn√©es a√©ronautiques (a√©roports, espaces a√©riens, obstacles) provenant d'OpenAIP. Le projet utilise une architecture medallion (RAW ‚Üí PROCESSED ‚Üí GOLD) avec Apache Spark pour le traitement et Grafana pour la visualisation.

## Architecture

```
NiFi ‚Üí Kafka ‚Üí Spark Processing ‚Üí MongoDB ‚Üí API REST ‚Üí Grafana
```

**Technologies utilis√©es :**
- **Apache NiFi** : Ingestion de donn√©es
- **Apache Kafka** : Streaming de messages
- **Apache Spark** : Traitement distribu√©
- **MongoDB** : Stockage des donn√©es
- **Node.js API** : Exposition REST des donn√©es
- **Grafana** : Visualisation

## Collections MongoDB

### RAW Layer (Donn√©es brutes)
- `airports_raw` : A√©roports bruts de Kafka
- `airspaces_raw` : Espaces a√©riens bruts
- `obstacles_raw` : Obstacles bruts

### PROCESSED Layer (Donn√©es nettoy√©es)
- `airports_processed` : A√©roports avec coordonn√©es pars√©es
- `airspaces_processed` : Espaces a√©riens structur√©s
- `obstacles_processed` : Obstacles structur√©s

### GOLD Layer (Analytics)
- `airport_airspaces` : A√©roports √† proximit√© (<30km) d'espaces a√©riens
- `airport_obstacles` : A√©roports √† proximit√© (<5km) d'obstacles

## üöÄ Installation et Lancement

### Pr√©requis

- Docker & Docker Compose
- 8 GB RAM minimum
- Ports disponibles : 3100, 5000, 7177, 8180, 9092, 27017

### √âtape 1 : D√©marrer l'infrastructure

```powershell
# D√©marrer tous les services Docker
docker-compose up -d

# V√©rifier que tous les conteneurs sont d√©marr√©s
docker-compose ps
```

**Services lanc√©s :**
- Zookeeper (port 2181)
- Kafka (port 9092)
- MongoDB (port 27017)
- Spark Master (port 8180)
- Spark Workers
- NiFi (port 8443)
- Grafana (port 3100)
- API REST (port 5000)

### √âtape 2 : V√©rifier MongoDB

```powershell
# V√©rifier la connexion MongoDB
docker-compose exec mongodb mongosh -u admin -p admin --authenticationDatabase admin
```

Dans le shell MongoDB :
```javascript
show dbs
use openaip_data
show collections
exit
```

### √âtape 3 : Ex√©cuter le traitement Spark

```powershell
# Copier le script de traitement dans le conteneur docker (si pas encore fait)
docker cp spark_scripts/openaip_processing.py projet_m2_data-spark-master-1:/tmp/openaip_processing.py

# Entrer dans le conteneur Spark Master
docker-compose exec spark-master bash

# Lancer le script de traitement (dans le conteneur)
/opt/spark/bin/spark-submit --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.2,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 /opt/spark/work-dir/openaip_processing.py 

# Sortir du conteneur
exit
```

**R√©sultat attendu :**
```
üì° Ingestion Kafka ‚Üí RAW: airports_raw
üì° Ingestion Kafka ‚Üí RAW: airspaces_raw
üì° Ingestion Kafka ‚Üí RAW: obstacles_raw
üü® Building GOLD datasets...
‚úÖ GOLD done
‚úÖ Pipeline COMPLETED ‚Äî RAW + PROCESSED + GOLD written to MongoDB
```

### √âtape 4 : V√©rifier l'API REST

```powershell
# Tester l'API
curl http://localhost:5000/

# V√©rifier les donn√©es des a√©roports
curl http://localhost:5000/api/airports_processed
```

**Endpoints disponibles :**
- `GET /api/airports_processed`
- `GET /api/airspaces_processed`
- `GET /api/obstacles_processed`
- `GET /api/airport_airspaces`
- `GET /api/airport_obstacles`

### √âtape 5 : Visualisation sur Grafana
M√©thode 1:utilisation de JsonApi

Methode 2:Installer le plugin Grafana

```powershell
# Installer le plugin Infinity
docker-compose exec grafana grafana-cli plugins install yesoreyeram-infinity-datasource

# Red√©marrer Grafana
docker-compose restart grafana
```

Attendre 15 secondes pour que Grafana red√©marre.

### √âtape 6 : Configurer Grafana

#### A. Acc√©der √† Grafana
- URL : `http://localhost:3100`
- Username : `admin`
- Password : `admin`

#### B. Ajouter la Data Source

1. **Menu** ‚Üí **‚öôÔ∏è Configuration** ‚Üí **Data Sources**
2. **Add data source** ‚Üí Chercher **"Infinity"** (si plugging install√©) sinon ‚Üí Chercher **JsonAPI**

3. Configuration (pour Infinity) :
   ```
   Name: OpenAIP API
   ```
4. **Save & Test**

#### C. Cr√©er un Dashboard avec Geomap

1. **+ Create** ‚Üí **Dashboard** ‚Üí **Add visualization**
2. S√©lectionner **"OpenAIP API"**
3. S√©lectionner **"Geomap"**

#### D. Configuration de la requ√™te

Dans le panneau de requ√™te :

```
Query Type:     JSON
Parser:         Backend
Source:         URL
Format:         Table
URL:            http://api-mongo:3001/api/airports_processed
Method:         GET
```

**Colonnes** (Parsing options & Result fields) :

| Selector | Alias | Type |
|----------|-------|------|
| `latitude` | `latitude` | `Number` |
| `longitude` | `longitude` | `Number` |
| `name` | `name` | `String` |
| `country` | `country` | `String` |

#### E. Configuration du Geomap

Dans le panneau de droite :

**Panel options :**
```
Title: Carte des A√©roports OpenAIP
```

**Map view :**
```
View: Fit data
```

**Data layer ‚Üí Location :**
```
Location mode: Coords
Latitude field: latitude
Longitude field: longitude
```

**Marker :**
```
Size: 5
Color: Bleu
```

**Tooltip :**
```
Tooltip mode: Details
```

5. **Apply** ‚Üí **Save dashboard**
   - Dashboard name : `OpenAIP - Visualisation A√©ronautique`

## üéØ R√©sultat

Vous devriez voir :
- üó∫Ô∏è Une carte interactive du monde
- ‚úàÔ∏è Des marqueurs pour chaque a√©roport
- üìç D√©tails au survol (nom, pays, coordonn√©es)
- üîç Zoom et navigation possibles

## üìÅ Structure du Projet

```
projet_M2_Data/
‚îú‚îÄ‚îÄ docker-compose.yml              # Orchestration des services
‚îú‚îÄ‚îÄ spark_scripts/
‚îÇ   ‚îî‚îÄ‚îÄ openaip_processing.py       # Script Spark de traitement
‚îú‚îÄ‚îÄ api_mongo/
‚îÇ   ‚îú‚îÄ‚îÄ server.js                   # API REST Node.js
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ infrastructure/
‚îÇ   ‚îî‚îÄ‚îÄ spark/
‚îÇ       ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ       ‚îî‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ nifi_templates/
    ‚îî‚îÄ‚îÄ nifi_version_finale.xml
```

## üîß Script Spark - D√©tails

Le script `openaip_processing.py` effectue :

### 1. RAW Layer
- Lit les topics Kafka : `openAIP_airports`, `openAIP_airspaces`, `openAIP_obstacles`
- Stocke les JSON bruts dans MongoDB
- D√©duplique par contenu

### 2. PROCESSED Layer
- Parse le JSON en colonnes structur√©es
- Convertit les coordonn√©es en `DoubleType`
- Parse les champs imbriqu√©s (`elevation`, `height`)
- D√©duplique par `name`

### 3. GOLD Layer
- **airport_airspaces** : Calcule les a√©roports √† <30km d'espaces a√©riens (formule Haversine)
- **airport_obstacles** : Calcule les a√©roports √† <5km d'obstacles

## üõ†Ô∏è Commandes Utiles

### Gestion des conteneurs

```powershell
# Voir les logs
docker-compose logs -f api-mongo
docker-compose logs -f spark-master

# Red√©marrer un service
docker-compose restart grafana
docker-compose restart api-mongo

# Arr√™ter tout
docker-compose down

# Tout supprimer (y compris les volumes)
docker-compose down -v
```

### MongoDB

```powershell
# Se connecter √† MongoDB
docker-compose exec mongodb mongosh -u admin -p admin --authenticationDatabase admin

# Voir les collections
use openaip_data
show collections

# Compter les documents
db.airports_processed.countDocuments()
db.airport_airspaces.countDocuments()
```

### V√©rifier les donn√©es

```powershell
# Via l'API
curl http://localhost:5000/api/airports_processed | ConvertFrom-Json | Select-Object -First 5

# Nombre d'a√©roports
(curl http://localhost:5000/api/airports_processed | ConvertFrom-Json).Count
```

## üêõ D√©pannage

### L'API ne d√©marre pas
```powershell
# Reconstruire l'image
docker-compose build api-mongo
docker-compose up -d api-mongo
```

### Spark ne trouve pas spark-submit
```powershell
# Utiliser le chemin complet
/opt/spark/bin/spark-submit ...
```

### Grafana ne montre pas les donn√©es
1. V√©rifier que l'API fonctionne : `curl http://localhost:5000/api/airports_processed`
2. V√©rifier que le plugin Infinity est install√©
3. Utiliser `http://api-mongo:3001` (pas `localhost:5000`) dans Grafana
4. V√©rifier que les champs sont bien de type `Number`

### MongoDB est vide
```powershell
# Relancer le script Spark
docker-compose exec spark-master /opt/spark/bin/spark-submit --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.2,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 /opt/spark/work-dir/openaip_processing.py
```

## üìù Notes Importantes

1. **R√©seau Docker** : L'API est accessible via `api-mongo:3001` depuis Grafana (r√©seau interne) et `localhost:5000` depuis Windows
2. **Donn√©es** : Le script Spark √©crase les collections √† chaque ex√©cution (mode `overwrite`)
3. **Performance** : Le traitement Spark peut prendre 1-2 minutes selon la quantit√© de donn√©es
4. **Plugins Grafana** : Le plugin Infinity est essentiel pour la visualisation

## üéì Auteur

Projet M2 Data - Donn√©es Distribu√©es
L√©o Charles
Audrey Magne

## üìÖ Date

Octobre 2025
